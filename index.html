<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Introduction to In-Context Learning - Hsiang Fu, Heni Ben Amor">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Hsiang Fu, Heni Ben Amor">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Interactive Robotics Lab, Arizona State University">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Introduction to In-Context Learning">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Introduction to In-Context Learning (ASU Interactive Robotics Lab)</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "Hsiang Fu",
        "affiliation": {
          "@type": "Interactive Robotics Lab",
          "name": "Arizona State University"
        }
      },
      {
        "@type": "Person",
        "name": "Heni Ben Amor",
        "affiliation": {
          "@type": "Interactive Robotics Lab",
          "name": "Arizona State University"
        }
      }
    ]

    "datePublished": "2025-10-31",
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="nerd" >
    <div class="nerd-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Introduction to In-Context Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/hsiangfu" target="_blank" style="color: #8C1D40 !important;">Hsiang Fu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://henibenamor.weebly.com/" target="_blank" style="color: #8C1D40 !important;">Heni Ben Amor</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://mohamedelmistiri.github.io/" target="_blank" style="color: #8C1D40 !important;">Mohamed El Mistiri</a><sup>1</sup></span>
            </div>
                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup><a href="https://interactive-robotics.engineering.asu.edu/" target="_blank" style="color: #4a4a4a !important; font-weight: normal !important;">Interactive Robotics Lab, Arizona State University</a></span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://github.com/hsiang-fu/icl_demos" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="https://drive.google.com/drive/folders/1NFf7mnk8v5Z3A2JeIBdeNt-ifKLDQEXW?usp=sharing" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-google-drive"></i>
                      </span>
                      <span>Demos</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span> -->
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://interactive-robotics.engineering.asu.edu/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-flask"></i>
                  </span>
                  <span>Lab</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-white">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-sevenths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified" style="align-items: stretch;">
          <p>
            In-Context Learning (ICL) is a powerful method that enables large language models (LLMs) to perform new tasks without altering any of their internal parameters, such as weights or biases. Unlike traditional machine learning, which requires explicit retraining, gradient updates, or fine-tuning on new datasets, ICL works entirely through the input prompt. By embedding instructions, examples, or demonstrations directly in the input text, we guide the model to infer the underlying pattern and adapt its output behavior dynamically. In this sense, the model‚Äôs ‚Äúlearning‚Äù happens in real time, during the interaction, rather than through an offline training step<sup>[2]</sup>.
          </p>
          <p>
            One way to understand ICL is to think of it as ‚Äúlearning by showing.‚Äù When we present the model with a few input‚Äìoutput pairs that illustrate the task, the model analyzes the structure of the examples‚Äîhow the inputs are transformed, what relationships matter, and what rules appear to be operating. Then, when faced with a new but related query, it extends or completes the pattern it has detected. For instance, if we show a few examples of converting informal text into formal prose, labeling sentiment, extracting key information, or solving a math pattern, the model can immediately generalize the behavior even with only a handful of samples.
          </p>
          <p>
            A useful analogy is a child observing a simple block-stacking pattern. After watching someone arrange the blocks a few times, the child does not need a full explanation of physics or geometry to replicate the behavior‚Äîthey simply infer the pattern from the demonstrations. Similarly, LLMs rely on their vast pre-training knowledge to interpret the examples we provide and map them to the task at hand. The examples in the prompt act as contextual training data, and the model leverages its internal representations to align with those examples, even though its core parameters remain unchanged.
          </p>
          <p>Since ICL requires no training pipeline, it offers flexibility and efficiency. It allows rapid iteration, adapts to a wide range of tasks, and supports few-shot or even zero-shot generalization. This capability is one of the reasons modern LLMs are so versatile: they can flexibly shift between tasks‚Äîfrom classification and reasoning to translation and summarization‚Äîsimply by receiving the right context.</p>
        </div>
        <br>
        <h2 class="title is-3">How does In-Context Learning Work?</h2>
        <div class="content has-text-justified" style="align-items: stretch;">
          <p>
            LLMs learn from examples by reading them as part of the input. The model looks for patterns, relationships, or rules that connect the example inputs to their corresponding outputs. Once it detects these patterns, it applies them to new information that's provided.
          </p>

          <h3 class="title is-4">1. The Model Reads Examples Like a Story</h3>
          <p>
            When we give a sequence of examples, the model processes them just like reading sentences. For example:
          </p>
          <pre><code>1 ‚Üí 2<br>2 ‚Üí 4<br>3 ‚Üí 6<br>What about 4?</code></pre>
          <p>
            The model sees the pattern‚Äîmultiplying by two‚Äîand uses it to continue.
          </p>

          <h3 class="title is-4">2. The Model Detects Patterns</h3>
          <p>
            Thanks to the massive pretraining of LLMs, the model develops strong pattern recognition abilities. During ICL, it uses those abilities to identify similarities across examples and infer the rule being demonstrated. These rules can involve numbers, text transformations, reasoning steps, or even abstract visual structures.
          </p>

          <h3 class="title is-4">3. The Model Completes the Pattern</h3>
          <p>
            After recognizing the pattern, the model continues it. For example:
          </p>
          <pre>Translate to French: <br> cat ‚Üí chat <br> dog ‚Üí chien <br> bird ‚Üí oiseau <br> horse ‚Üí ?</pre>
          <p>
            The model responds with the appropriate continuation of the pattern.
          </p>

          <h3 class="title is-4">4. No Training Required</h3>
          <p>
            Most importantly, the model isn‚Äôt being retrained. Its parameters stay the same. ICL works because the model uses its existing knowledge to match the patterns shown in the examples, making it feel adaptive even without any weight updates.
          </p>
        </div>

        <h2 class="title is-3">Simple Analogy</h2>
        <div class="content has-text-justified" style="align-items: stretch;">
          <p>
            Imagine showing a child three different drawings:
          </p>
          <pre>üçé + üçé = 2 apples <br>üçå + üçå = 2 bananas <br>üçä + üçä = 2 oranges</pre>
          <p>
            If you then show:
          </p>
          <pre>üçá + üçá = ?</pre>
          <p>
            The child would be able to easily continue the same pattern: <strong>2 grapes</strong><br>
            ICL works the same way: show, observe, follow.
          </p>
        </div>
        <br>
        <h2 class="title is-3">Why Is In-Context Learning Important?</h2>
        <div class="content has-text-justified" style="align-items: stretch;">
          <p><b>Let's compare traditional ML approaches with in-context learning</b></p>
          <h3 class="title is-4">Traditional Machine Learning: ‚ÄúLearn First, Use Later‚Äù</h3>
          <p>
            In traditional machine learning, the process is divided into two major phases. The first phase is
            <strong>training</strong>, where the model uses a large training dataset to adjust‚Äîand continually update‚Äîits
            internal parameters, such as weights and biases. This optimization process is computationally expensive and
            can take hours, days, or even weeks.
          </p>
          <figure>
            <img src="static/images/traditional_ml.png" alt="Traditional Machine Learning Diagram" style="width: 100%; max-width: 500px; height: auto;">
            <figcaption style="font-size: 15px;">Figure 1: Traditional ML involves two phases‚Äîtraining and inference.</figcaption>
          </figure>
          <b>Step 1: Training the Model</b>
          <ul>
            <li>A large training dataset is fed into the model.</li>
            <li>The model adjusts its internal parameters (weights and biases) over many cycles.</li>
            <li>The goal is to find the optimal set of parameters that allow the model to make good predictions.</li>
            <li>This step is slow, computationally heavy, and usually done only once before deployment.</li>
          </ul>
          <b>Think of this like a student studying for a final exam:</b>
          <ul><li>They ‚ë† read books, ‚ë° take notes, ‚ë¢ do practice problems, and ‚ë£ slowly update their understanding over time.</li></ul>
          <b>Step 2: Inference</b>
          <ul>
            <li>Once the model is fully trained, it receives a new input query.</li>
            <li>It does not change its parameters anymore.</li>
            <li>Instead, it uses the knowledge stored in its trained weights to produce a model output.</li>
            <li>The model relies solely on what it learned during the training phase.</li>
          </ul>
          <b>Think of this like a student taking a final exam:</b>
          <ul><li>‚ë† No more studying and ‚ë° the student must rely on what they learned earlier.</li></ul>

          <h3 class="title is-4">In-Context Learning: ‚ÄúLearn While You Read‚Äù</h3>
          <p>
            In contrast, In-Context Learning (ICL) allows a model to adapt its behavior <em>during</em> inference without
            modifying its internal parameters. Instead of retraining the model, we provide examples, instructions, or
            demonstrations directly inside the input prompt. These elements become the ‚Äúcontext‚Äù that guides the model‚Äôs
            reasoning.
          </p>
          <figure>
            <img src="static/images/icl.png" alt="In-Context Learning Diagram" style="width: 100%; max-width: 500px; height: auto;">
            <figcaption style="font-size: 15px;">Figure 2: ICL adapts behavior using context, with no parameter updates.</figcaption>
          </figure>

          <b>Instead of training the model again, we give it context directly inside the input, which includes:</b>
          <ul>
            <li>Examples</li>
            <li>Instructions</li>
            <li>Demonstrations</li>
            <li>The User's Query</li>
          </ul>
          <p>Then, the model uses pretrained knowledge (everything it learned during its original training), the examples we give in the prompt, and the current input query, combines them during inferenceto produce both an output and sometimes an explanation.</p>
          <p>Unlike traditional ML, ICL does not update the model‚Äôs weights, instead, everything happens ‚Äúin the moment,‚Äù inside the prompt window.</p>
          <b>This is like giving a student a cheat sheet during an exam:</b>
          <ul><li>‚ë† They don‚Äôt update their long-term memory but ‚ë° can follow the examples provided to solve the problem.</li></ul>
          <p>
            The model uses a combination of its pretrained knowledge, the examples provided in the prompt, and the user‚Äôs
            input query to generate both an output and, optionally, an explanation. Importantly, the model‚Äôs parameters
            remain unchanged. All adaptation happens dynamically inside the context window.
          </p>

          <h3 class="title is-4">Key Differences</h3>
          <ul>
            <li><strong>Traditional ML:</strong> Learning happens during training; inference just applies what was learned.</li>
            <li><strong>ICL:</strong> Learning-like behavior happens during inference using contextual examples.</li>
            <li><strong>Traditional ML:</strong> Requires updating parameters to learn new tasks.</li>
            <li><strong>ICL:</strong> No retraining needed‚Äîjust change the prompt.</li>
            <li><strong>Traditional ML:</strong> Adaptation is slow and computationally heavy.</li>
            <li><strong>ICL:</strong> Adaptation is instant and flexible.</li>
          </ul>
          <p>
            A useful way to think about the difference is this: Traditional ML requires ‚Äústudying ahead of time,‚Äù while
            ICL allows the model to ‚Äúlearn from examples in the moment.‚Äù
          </p>
          <h3 class="title is-4">Summary and Key Insights</h3>
          <p><b>ICL offers several advantage over traditional approaches:</b></p>
          <ul>
            <li><strong>Fast adaptation:</strong> The model can instantly perform new tasks.</li>
            <li><strong>No retraining risk:</strong> Nothing about the model‚Äôs core parameters changes.</li>
            <li><strong>Flexible across formats:</strong> Works with text, numbers, code, and even abstract patterns.</li>
            <li><strong>Efficient demonstrations:</strong> A few examples (even one!) can be enough.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper Overview -->
<section class="section hero is-white">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-sevenths">
        <h2 class="title is-3">A Simple In-Context Learning Demo (move to different website)</h2>
        <p>or</p>
        <br>
        <h2 class="title is-3">Getting Started with a Simple Demo</h2>
        
        <div class="content has-text-justified">
          <p>
            This demo<sup>1</sup> shows an example of how a large language model (LLM) can learn from examples to complete a pattern. It uses tasks from the Abstract and Reasoning Corpus<sup>2</sup> (ARC), which contain small grids that follow certain rules.
          </p>
          <p>
            For each task, we provide the model with three input‚Äìoutput examples that show how the pattern works. We then give it a new test input and ask the Gemini 2.5-Flash model to produce the correct output grid by following the same rule seen in the examples.
          </p>
          <p>
            In the example image below, you can use the arrows to browse through the input‚Äìoutput examples and see the model‚Äôs generated results.
          </p>
          <div style="text-align:center; margin-top: 1.5rem;">
              <figure style="max-width: 500px; margin: 0 auto;">
                <img id="displayedImage" src="static/images/sequence_transformation_1.png" alt="Sequence Transformation Examples" style="border-radius: 10px; object-fit: cover;">
              </figure>

              <div class="buttons is-centered" style="margin-top: 1rem;">
                <button class="button is-dark is-rounded" onclick="updateGlobalIndex(-1)">
                  <span class="icon"><i class="fa fa-arrow-left"></i></span>
                </button>

                <pre id="objectiveText" class="button is-static is-rounded" style="font-family: monospace;">Task One</pre>

                <button class="button is-dark is-rounded" onclick="updateGlobalIndex(1)">
                  <span class="icon"><i class="fa fa-arrow-right"></i></span>
                </button>
              </div>
            </div>
            <br>
            <div align="center">
              <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing" target="_blank" class="external-link button is-normal is-rounded is-dark" style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
                <span class="icon">
                  <i class="fa fa-file"></i>
                </span>
                <span>Try the Full Tutorial Here</span>
              </a>
            </div>
            <br>
          <div style="font-size: 0.75em;">
            <div><sup>1</sup> This demo is inspired by <i>Large Language Models as General Pattern Machines</i> [2], which examines how LLMs can identify and generalize visual and abstract patterns.</div>
            <div><sup>2</sup> <i>The ARC dataset</i> [1], introduced by Fran√ßois Chollet, is designed to evaluate abstract reasoning and few-shot generalization‚Äîkey components of human-like intelligence. ARC avoids dependence on prior knowledge, making it a strong test of an LLM‚Äôs ability to learn patterns from scratch.</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Overview -->

<!-- Paper Demos -->
<section class="section hero is-white">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Demos</h2>
    <div class="columns is-multiline">

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">Sequence Prediction</h3>
          <img src="static/images/sequence_completion_axsin(bx).gif" alt="a‚ãÖx‚ãÖsin(bx)">
          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">Glucose Level Prediction</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">Pattern Completion</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">Binary Classification</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">Image Classification</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">Movie Translation?</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">Linear Regression</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">SAS-Prompt</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">ProPS</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">ProPS+</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">PiD Tuning</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>

      <div class="column is-one-fourth">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4 has-text-centered">Numerical Optimization</h3>
          <p>image goes here</p>

          <br>
          <div align="center">
            <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
              style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
              <span class="icon">
                <i class="fa fa-file"></i>
              </span>
              <span>Try the Tutorial Here</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  <!-- Demos for all 12 blocks -->

  <br>
  <p>remove content below</p>
  <br>
  <br>
<section class="section hero is-white">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Demos (old) placeholders</h2>
    <div class="columns is-multiline is-variable is-6" style="align-items: stretch;">
      
      <!-- Left Column -->
      <div class="column is-half">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4">Sequence Prediction</h3>
          <p>
            This demo is inspired by <em>Large Language Models as General Pattern Machines</em> [2], which explores how Large Language Models (LLMs) can identify and generalize both visual and abstract patterns.
          </p>
          <br>
          <p>
            In this demo, we‚Äôre going to test how well an LLM can recognize and extend mathematical patterns, specifically those derived from the sine function. We can start by creating a set of numerical sequences based on the sine function ‚Äî specifically <strong>a‚ãÖx‚ãÖsin(bx)</strong> and <strong>a‚ãÖsin(bx)</strong>. For each function, we will generate 10 example sequences using x-values uniformly sampled from the range [0, œÄ].
          </p>
          <br>
          <p>
            Next, let‚Äôs feed these sequences into the <strong>Gemini 2.5-Flash</strong> model. We‚Äôll disable reasoning mode so the model focuses purely on continuing numerical patterns rather than performing abstract reasoning. Along with the training examples, we will also include a custom test sequence that goes slightly beyond the œÄ interval. This helps test whether the model can continue the sine-like behavior even outside the range it was ‚Äútrained‚Äù on, without the context or equation of the true sine function.
          </p>
          <br>
          <p>
            Finally, we can ask the model to generate 50, 100, or 200 additional values that continue the test sequence naturally. These steps and results demonstrate how the LLM can recognize, generalize, and extend sinusoidal patterns learned from the provided examples.
          </p>
          <br>
          <h3 class="title is-4">Prompt Example</h3>
<pre style="white-space: pre-wrap; word-wrap: break-word;">
<code>You are given several example sequences of (x, y) pairs generated by different mathematical patterns.

Example 1: [(0.0, 0.0), (0.025, 0.199), (0.05, 0.389), ...]
Example 2: [(0.0, 0.0), (0.025, 0.397), (0.05, 0.779), ...]
Example 3: [(0.0, 0.0), (0.025, 0.596), (0.05, 1.168), ...]
Example 4: [(0.0, 0.0), (0.025, 0.795), (0.05, 1.558), ...]
Example 5: [(0.0, 0.0), (0.025, 0.993), (0.05, 1.947), ...]
Example 6: [(0.0, 0.0), (0.025, 1.192), (0.05, 2.337), ...]
...

The following sequence represents a partial test input: {test}

Now generate the next 200 new (x, y) pairs that follow the same underlying mathematical pattern, continuing naturally from where the test sequence ends.

Output a  Python list of [x, y] pairs in this format, remember to close all brackets correctly:
[(x1, y1), (x2, y2), (x3, y3), (x4, y4), (x5, y5), ...]
No explanations, no code, no comments ‚Äî only the list.
</code>
</pre>
        </div>
      </div>

      <!-- Right Column -->
      <div class="column is-half">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4">Example of Demo</h3>
            <img src="static/images/sequence_completion_axsin(bx).gif" alt="a‚ãÖx‚ãÖsin(bx)">
              <p>
              For the function <strong>a‚ãÖx‚ãÖsin(bx)</strong>, the initial example we provide to the model is shown by the gray curve, which represents the known part of the sequence. We then ask the model to predict the next 50, 100, or 200 values beyond the known range‚Äîshown by the red curve. The dashed gray curve represents the ground-truth continuation of the pattern.
            </p>
            <br>
            <img src="static/images/sequence_completion_asin(bx).gif" alt="a‚ãÖsin(bx)">
              <p>
              For the function <strong>a‚ãÖsin(bx)</strong>, the initial example we provide to the model is shown by the gray curve, which represents the known part of the sequence. We then ask the model to predict the next 50, 100, or 200 values beyond the known range‚Äîshown by the red curve. The dashed gray curve represents the ground-truth continuation of the pattern.
              </p>
            <br>
              <p>
              In the demo, you can adjust the frequency and amplitude settings when prompted, allowing you to see how the LLM interprets the underlying mathematical pattern and predicts future values based on the provided examples.
            </p>
            <br>
              <div align="center">
              <a href="https://colab.research.google.com/drive/1GSsmx1Ms5-RnfafRm6qGohgaVgyrSi0W?usp=sharing" target="_blank" class="external-link button is-normal is-rounded is-dark" style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
                <span class="icon">
                  <i class="fa fa-file"></i>
                </span>
                <span>Try the Tutorial Here</span>
              </a>
            </div>
        </div>
      </div>

      <!-- Left Column -->
      <div class="column is-half">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4">Glucose Level Prediction</h3>
          <p>
            In this demo, we utilize a collection of full daily time-series sequences‚Äîeach containing paired glucose measurements and meal logs‚Äîas the ‚Äútraining‚Äù examples for our Large Language Model (LLM). These sequences capture the natural dependencies between meals and the resulting glucose responses, giving the model a rich foundation to learn from.
          </p>
          <br>
          <p>
            Next, we run two prediction tasks: one with explicit Glycemic Index (GI) information and one without it. In both cases, the underlying sequence data stays the same; what changes is the prompt. For the GI-aware version, we augment the prompt with instructions telling the model to consider GI values when predicting future glucose levels. For the GI-agnostic version, we simply remove that guidance.
          </p>
          <br>
          <p>
            For both tasks, we feed the daily sequences and step-by-step context into the Gemini 2.0 Flash model. We keep reasoning mode turned off so the model focuses on direct pattern continuation rather than higher-level interpretation. Alongside the training examples, we also provide a test sequence representing a future day we want the model to predict.
          </p>
          <br>
          <p>
            To generate the full set of predictions for this target day, we ask the model to produce one value at a time, looping sequentially until the entire day's glucose curve is completed. This lets us evaluate how well the model can recognize, generalize, and extend the patterns it learned from the earlier sequences‚Äîboth with and without GI guidance.
          </p>
          <br>
          <p>
            Finally, we compare the LLM‚Äôs output to a Baseline Model (Gaussian Process Regression) and evaluate performance using Root Mean Squared Error (RMSE) and Mean Squared Error (MSE).
          </p>
          <br>
          <h3 class="title is-4">Prompt Example</h3>
          <pre style="white-space: pre-wrap; word-wrap: break-word;">
<code>You are the world's best glucose level predictor for adults.
Here is an example of my glucose level data for 20-11-2023:
Date           Time     Blood Sugar Level
20-11-2023  10:00 AM                 89
20-11-2023  10:50 AM                100
20-11-2023  12:38 AM                 91
20-11-2023   1:40 PM                 93
20-11-2023   3:03 PM                111
20-11-2023   3:37 PM                122
20-11-2023   4:12 PM                116
20-11-2023   6:15 PM                101
20-11-2023   7:27 PM                126
20-11-2023   9:27 PM                118
20-11-2023   9:49 PM                120
20-11-2023  10:26 PM                123
20-11-2023  10:47 PM                121
20-11-2023  11:47 PM                115

and examples of the food I consumed at the time:
Date          Time    Meal Type   Food Items  
20-11-2023  10:08 AM  Breakfast   Poha (150 gm), Tea (100 ml)
20-11-2023   1:53 PM      Lunch   Roti (4 pc), Baingan ...
20-11-2023   6:27 PM     Snacks   Samosa Chaat, Coca Cola ...
20-11-2023   8:47 PM     Dinner   Ghee Roti (3 pc) ...

My last glucose level was 118 at 6:24 PM
I just ate Popcorn, Potato Chips, Pepsi at 6:32 PM for Snacks on 23-11-2023, <strong><span style="color: red;">using the glycemic index</span></strong>, predict my glucose level at 8:02 PM

    or 

My last glucose level was 135 at 8:02 PM
I ate Popcorn, Potato Chips, Pepsi previously at 6:32 PM for Snacks on 16-11-2023, <strong><span style="color: red;">using the meal</span></strong>, predict my glucose level at 9:32 PM

*Note: I am not diabetic.

Output only a Python list with a single dictionary in this format:
[{{'Date': '{target_date}', 'Time': '{t}', 'Blood Sugar Level': value}}]
No extra text, no explanations, no code, only the list.
</code>
</pre>
        </div>
      </div>

      <!-- Right Column -->
      <div class="column is-half">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4">Example of Demo</h3>
            <img src="static/images/glucose_prediction_1.png" alt="w/ mention of Glycemic Index">
            <p>
              By <strong>incorporating</strong> the Glycemic Index, we guide Gemini-2.0-Flash to use the provided examples, the test sequence, and the associated food data to iteratively predict the patient‚Äôs glucose level at each subsequent time point. The model continues this step-by-step prediction process until it reaches the end of the day.
            </p>
            <br>
            <img src="static/images/glucose_prediction_2.png" alt="w/o mention of Glycemic Index">
              <p>
              By <strong>excluding</strong> the Glycemic Index, we prompt Gemini-2.0-Flash to rely solely on the provided examples, the test sequence, and the food data to iteratively predict the patient‚Äôs glucose level at each subsequent time point. The model continues this step-by-step prediction process until it reaches the end of the day.
            </p>
            <br>
            <img src="static/images/glucose_prediction_3.png" alt="Baseline Gaussian Process Regression Model">
            <p>
              For the <strong>Baseline Gaussian Process Regression</strong> (GPR) model, we generate predictions using only the training data, which consist of the patient‚Äôs glucose measurements. The model then forecasts glucose levels throughout the day at predefined time points.
            </p>
            <br>
            <p>
              In the demo, you can select the number of days to include as examples in the demo, and explore how LLMs leverage food-related data, in combination with semantic cues, to predict future values in time-dependent datasets based on the provided examples.
            </p>
            <br>
            <div align="center">
              <a href="https://colab.research.google.com/drive/1H9d4L6nRtwdMAzK74TMEHPhXP1jiWOwB?usp=sharing" target="_blank" class="external-link button is-normal is-rounded is-dark" style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
                <span class="icon">
                  <i class="fa fa-file"></i>
                </span>
                <span>Try the Demo Here @ Google Colab</span>
              </a>
            </div>
          
        </div>
      </div>

      <div class="column is-half">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4">Seqeunce Transformation</h3>
          <p>
            This demo is inspired by the concepts presented in <em>Large Language Models as General Pattern Machines</em> [2], which explores the capacity of Large Language Models (LLMs) to recognize and generalize visual and abstract patterns.
          </p>
          <br
          <p>
            In this demo, we utilized the Abstract and Reasoning Corpus (ARC) as the source of pattern-based examples for the Large Language Model (LLM). For the ARC tasks, the model was provided with three input‚Äìoutput examples, visually capturing the transformation rules required to solve the task.
          </p>
          <br>
          <p>
            The input-output examples were then input into the Gemini 2.5-Flash model, with reasoning mode turned off to focus on direct pattern recognition and application rather than higher-level inference. In addition to the three to four training examples, a test input pattern was provided to assess the model‚Äôs ability to extrapolate and apply the observed visual transformation rules to a novel case. Finally, the model was prompted to generate the single corresponding output grid for the test input, demonstrating the capacity of LLMs to recognize, generalize, and execute the learned visual transformation pattern.
          </p>
          <br>
          <p>
            The Abstraction and Reasoning Corpus (ARC) is a distinctive benchmark introduced in 2019 by Google AI researcher Fran√ßois Chollet to measure AI skill acquisition and progress toward human-level artificial general intelligence (AGI). Unlike traditional benchmarks, ARC assesses abstract reasoning and the ability to generalize from just a few examples , which mirrors the way humans quickly learn new concepts. It stands apart because it does not rely on specific, pre-trained knowledge, making it a robust test of an AI's fluid intelligence [1].
          </p>
        </div>
      </div>

      <!-- Right Column -->
      <div class="column is-half">
        <div class="box has-text-justified" style="height: 100%;">
          <h3 class="title is-4">Example of Demo</h3>
            <div style="text-align:center; margin-top: 1.5rem;">
              <figure style="max-width: 500px; margin: 0 auto;">
                <img id="displayedImage" src="static/images/sequence_transformation_1.png" alt="Sequence Transformation Examples" style="border-radius: 10px; object-fit: cover;">
              </figure>

              <div class="buttons is-centered" style="margin-top: 1rem;">
                <button class="button is-dark is-rounded" onclick="updateGlobalIndex(-1)">
                  <span class="icon"><i class="fa fa-arrow-left"></i></span>
                </button>

                <pre id="objectiveText" class="button is-static is-rounded" style="font-family: monospace;">Task One</pre>

                <button class="button is-dark is-rounded" onclick="updateGlobalIndex(1)">
                  <span class="icon"><i class="fa fa-arrow-right"></i></span>
                </button>
              </div>
            </div>
            <br>
            <p>
            Users can select the patterns/tasks they would like <strong>Gemini-2.5 Flash</strong> to transform or complete, and explore how this powerful LLM leverages simple input-output examples, in combination with its linguistic understanding, to generalize the underlying rule and generate accurate outputs based on the pattern learned from those examples.
          </p>
            <br>
            <div align="center">
              <a href="https://colab.research.google.com/drive/1buA3TM0VWCR_8x3KT0ZzQnt0uEt3poIa?usp=sharing" target="_blank" class="external-link button is-normal is-rounded is-dark" style="box-shadow: 0 4px 8px rgba(0,0,0,0.25);">
                <span class="icon">
                  <i class="fa fa-file"></i>
                </span>
                <span>Try the Demo Here @ Google Colab</span>
              </a>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Paper Demos -->

<!--References-->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-left">
          <p style="text-indent: -2em; font-size: 0.85em;">
            [1] Chollet, F. (2019).
            On the Measure of Intelligence. <em>arXiv preprint arXiv:1911.01547</em>.
            <a href="https://arxiv.org/abs/1911.01547" target="_blank">https://arxiv.org/abs/1911.01547</a>
          </p>
          <p style="text-indent: -2em; font-size: 0.85em;">
            [2] Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M. G., Rao, K., Sadigh, D., & Zeng, A. (2023).
            Large Language Models as General Pattern Machines. <em>arXiv preprint arXiv:2307.04721</em>.
            <a href="https://arxiv.org/abs/2307.04721" target="_blank">https://arxiv.org/abs/2307.04721</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End References-->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

<script>
  const objectives = [
    { name: "One", image: "static/images/sequence_transformation_1.png" },
    { name: "Two", image: "static/images/sequence_transformation_2.png" },
    { name: "Three", image: "static/images/sequence_transformation_3.png" },
    { name: "Four", image: "static/images/sequence_transformation_4.png" },
  ];

  let currentIndex = 0;

  function updateGlobalIndex(direction) {
    currentIndex = (currentIndex + direction + objectives.length) % objectives.length;
    const { name, image } = objectives[currentIndex];
    document.getElementById("displayedImage").src = image;
    document.getElementById("objectiveText").textContent = `Task ${name}`;
  }
</script>

  </body>
  </html>
